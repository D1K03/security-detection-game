from __future__ import annotations

from datetime import datetime
from typing import List, Literal, Optional

from pydantic import BaseModel, Field

Difficulty = Literal["easy", "medium", "hard"]
VulnType = Literal["xss", "sqli", "ssrf", "rce", "none"]

# Model for difficulty configuration
class DifficultyConfig(BaseModel):
    base_time_seconds: int # Base time allocated per task
    penalty_seconds: int # Time penalty for incorrect answers
    vuln_density: float # Proportion of tasks that are vulnerable
    complexity_tag: Literal["low", "medium", "high"] # Complexity level of tasks


#configurations based on the difficulty level 
DIFFICULTY_CONFIGS: dict[Difficulty, DifficultyConfig] = {
    "easy": DifficultyConfig(
        base_time_seconds=120,
        penalty_seconds=5,
        vuln_density=0.4,
        complexity_tag="low",
    ),
    "medium": DifficultyConfig(
        base_time_seconds=90,
        penalty_seconds=8,
        vuln_density=0.6,
        complexity_tag="medium",
    ),
    "hard": DifficultyConfig(
        base_time_seconds=60,
        penalty_seconds=12,
        vuln_density=0.8,
        complexity_tag="high",
    ),
}


# model for tasks generated by claude 
class TaskSchema(BaseModel):
    id: str
    code: str
    is_vulnerable: bool
    vulnerability_type: VulnType
    difficulty: Difficulty


class TaskPublicSchema(BaseModel):
    id: str
    code: str
    difficulty: Difficulty


class SessionCreateRequest(BaseModel):
    difficulty: Difficulty
    task_count: int = Field(ge=1, le=10)

# response model for session creation with the configured parameters based on difficulty
class SessionCreateResponse(BaseModel):
    session_id: str
    created_at: datetime
    difficulty: Difficulty
    config: DifficultyConfig

#response model for listing tasks in a session
class TaskListResponse(BaseModel):
    session_id: str
    tasks: List[TaskPublicSchema]

#schema for each answer submitted by user
class AnswerSchema(BaseModel):
    task_id: str
    user_choice: Literal["clean", "sabotaged"]

#request model for submitting answers
class SubmitAnswersRequest(BaseModel):
    answers: List[AnswerSchema]

#final response after submitting answers
class SubmitAnswersResponse(BaseModel):
    correct: int
    incorrect: int
    missed_task_ids: List[str]

#audit log from hacktron 
class AuditLogSchema(BaseModel):
    task_id: str
    raw_log: str
    findings: Optional[List[str]] = None

# Mentor report schema from claude 
class MentorReportSchema(BaseModel):
    summary: str

#final response with claude report
class FinishResponse(BaseModel):
    session_id: str
    score: int
    missed_task_ids: List[str]
    audit_logs: List[AuditLogSchema]
    mentor_report: MentorReportSchema
